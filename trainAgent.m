%% create agent
agentOpts = rlDDPGAgentOptions(...
    "SampleTime",sampleTime,...
    "TargetSmoothFactor",1e-3,...
    "DiscountFactor",0.995, ...
    "MiniBatchSize",128, ...
    "ExperienceBufferLength",1e6); 

agentOpts.NoiseOptions.Variance = 0.1;
agentOpts.NoiseOptions.VarianceDecayRate = 1e-5;

obstacleAvoidanceAgentTarget = rlDDPGAgent(actor,critic,agentOpts);

%% use a pretrained agent as a starting point
load('data/pretrained_agent.mat');
%%
maxEpisodes = 10000;
maxSteps = ceil(Tfinal/sampleTime);
trainOpts = rlTrainingOptions(...
    "MaxEpisodes",maxEpisodes, ...
    "MaxStepsPerEpisode",maxSteps, ...
    "ScoreAveragingWindowLength",50, ...
    "StopTrainingCriteria","EpisodeCount", ...
    "StopTrainingValue",500, ...
    "SaveAgentCriteria", "EpisodeReward", ...
    "SaveAgentValue", 500, ...
    'SaveAgentDirectory', pwd + "\train\Agents", ...
    "Verbose", true, ...
    "Plots","training-progress");

trainingStats = train(obstacleAvoidanceAgentTarget,env,trainOpts);